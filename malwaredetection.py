import pandas as pd
import numpy as np
import pefile
import os
import hashlib
import array
import math
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.metrics import plot_confusion_matrix

MalwareDataset = pd.read_csv('MalwareFiles.csv', sep='|')        # loads the dataset using | seperator
Legit = MalwareDataset[0:41323].drop(['legitimate'], axis=1)    # Divides the dataset in legitimate samples
Malware = MalwareDataset[41323::].drop(['legitimate'], axis=1)  # Divides the dataset in Malware samples
MalwareDataset.head()

print('The total number of features are %i \n' % Legit.shape[1])      # Checks the number of important features in datset
Data = MalwareDataset.drop(['Name', 'md5', 'legitimate'], axis=1).values #takes out the unimportant features
Target = MalwareDataset['legitimate'].values                    # finds values to fit the model
FeatSelect = sklearn.ensemble.ExtraTreesClassifier().fit(Data, Target) #randomizes data to minimize overfitting
print(FeatSelect.feature_importances_)                          # prints the Tree calssifier important features
Model = SelectFromModel(FeatSelect, prefit=True)             # features scalling to discard the unimportant features or PCA
Data_new = Model.transform(Data)                                # featues scalling using z-score
Features = Data_new.shape[1]                                           # gets the number of important features after PCA
Index = np.argsort(FeatSelect.feature_importances_)[::-1][:Features]   # Indexes the important features
i = 1                                                                  # loop counter
for feat  in range(Features):                                          # loop in number of important features
    print(i,MalwareDataset.columns[2+Index[feat]])                     # adds two becuase we deleted the first two features
    i = i+1                                                            # loop increment
feature1 = MalwareDataset.columns[2+Index[0]]
feature2 = MalwareDataset.columns[2+Index[1]]
plt.figure(figsize=(10,7))
sns.scatterplot(x = feature1, y = feature2, s = 70, hue ='legitimate', data=MalwareDataset) # hue represents color
Legit_Train, Legit_Test, Malware_Train, Malware_Test = train_test_split(Data, Target ,test_size=0.2) 

 # Splits the data for training & testing                 
clf = RandomForestClassifier(n_estimators=50)                    # builds multiple trees model using n_estimators
clf.fit(Legit_Train, Malware_Train)                              # trains the model using on training dataset
score = clf.score(Legit_Test, Malware_Test)                     # gets the accuracy using test dataset 
print("The score of Random Forest Algorithm is", score*100)     # multiplying by 100 to get the percentage
Result = clf.predict(Legit_Test)                                # tests for geting the confusion matrix
CM = confusion_matrix(Malware_Test, Result)                     # confusion matrix 
print("False positive rate : %f %%" % ((CM[0][1] / float(sum(CM[0])))*100))
print('False negative rate : %f %%' % ( (CM[1][0] /float(sum(CM[1]))*100)))
plot_confusion_matrix(clf, Legit_Test, Malware_Test,cmap=plt.cm.Blues)  # plots the confusion matrix 

def get_entropy(data):
    if len(data) == 0:
        return 0.0
    occurences = array.array('L', [0]*256)
    print(occurences)
    for x in data:
        occurences[x if isinstance(x, int) else ord(x)] += 1

    entropy = 0
    for x in occurences:
        if x:
            p_x = float(x) / len(data)
            entropy -= p_x*math.log(p_x, 2)

    return entropy

def get_resources(pe):
    """Extract resources :
    [entropy, size]"""
    resources = []
    if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
        try:
            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:
                if hasattr(resource_type, 'directory'):
                    for resource_id in resource_type.directory.entries:
                        if hasattr(resource_id, 'directory'):
                            for resource_lang in resource_id.directory.entries:
                                data = pe.get_data(resource_lang.data.struct.OffsetToData, resource_lang.data.struct.Size)
                                size = resource_lang.data.struct.Size
                                entropy = get_entropy(data)

                                resources.append([entropy, size])
        except Exception as e:
            return resources
    return resources

def get_version_info(pe):
    """Return version infos"""
    res = {}
    for fileinfo in pe.FileInfo:
        if fileinfo.Key == 'StringFileInfo':
            for st in fileinfo.StringTable:
                for entry in st.entries.items():
                    res[entry[0]] = entry[1]
        if fileinfo.Key == 'VarFileInfo':
            for var in fileinfo.Var:
                res[var.entry.items()[0][0]] = var.entry.items()[0][1]
    if hasattr(pe, 'VS_FIXEDFILEINFO'):         
          res['flags'] = pe.VS_FIXEDFILEINFO.FileFlags
          res['os'] = pe.VS_FIXEDFILEINFO.FileOS
          res['type'] = pe.VS_FIXEDFILEINFO.FileType
          res['file_version'] = pe.VS_FIXEDFILEINFO.FileVersionLS
          res['product_version'] = pe.VS_FIXEDFILEINFO.ProductVersionLS
          res['signature'] = pe.VS_FIXEDFILEINFO.Signature
          res['struct_version'] = pe.VS_FIXEDFILEINFO.StrucVersion
    return res

def extract_infos(fpath):
    res = []
    pe = pefile.PE(fpath)
    res.append(pe.FILE_HEADER.Machine)                                            # Machine
    res.append(pe.FILE_HEADER.SizeOfOptionalHeader)                               # SizeOfOptionalHeader
    res.append(pe.FILE_HEADER.Characteristics)                                    # Characteristics
    res.append(pe.OPTIONAL_HEADER.MajorLinkerVersion)                             # MajorLinkerVersion
    res.append(pe.OPTIONAL_HEADER.MinorLinkerVersion)                             # MinorLinkerVersion
    res.append(pe.OPTIONAL_HEADER.SizeOfCode)                                     # SizeOfCode
    res.append(pe.OPTIONAL_HEADER.SizeOfInitializedData)                          # SizeOfInitializedData
    res.append(pe.OPTIONAL_HEADER.SizeOfUninitializedData)                        # SizeOfUninitializedData
    res.append(pe.OPTIONAL_HEADER.AddressOfEntryPoint)                            # AddressOfEntryPoint
    res.append(pe.OPTIONAL_HEADER.BaseOfCode)                                     # BaseOfCode
    try:
        res.append(pe.OPTIONAL_HEADER.BaseOfData)                                 # BaseOfData
    except AttributeError:
        res.append(0) 
    res.append(pe.OPTIONAL_HEADER.ImageBase)                                      # ImageBase
    res.append(pe.OPTIONAL_HEADER.SectionAlignment)                               # SectionAlignment
    res.append(pe.OPTIONAL_HEADER.FileAlignment)                                  # FileAlignment
    res.append(pe.OPTIONAL_HEADER.MajorOperatingSystemVersion)                    # MajorOperatingSystemVersion
    res.append(pe.OPTIONAL_HEADER.MinorOperatingSystemVersion)                    # MinorOperatingSystemVersion
    res.append(pe.OPTIONAL_HEADER.MajorImageVersion)                              # MajorImageVersion
    res.append(pe.OPTIONAL_HEADER.MinorImageVersion)                              # MinorImageVersion
    res.append(pe.OPTIONAL_HEADER.MajorSubsystemVersion)                          # MajorSubsystemVersion
    res.append(pe.OPTIONAL_HEADER.MinorSubsystemVersion)                          # MinorSubsystemVersion
    res.append(pe.OPTIONAL_HEADER.SizeOfImage)                                    # SizeOfImage
    res.append(pe.OPTIONAL_HEADER.SizeOfHeaders)                                  # SizeOfHeaders
    res.append(pe.OPTIONAL_HEADER.CheckSum)                                       # CheckSum
    res.append(pe.OPTIONAL_HEADER.Subsystem)                                      # Subsystem
    res.append(pe.OPTIONAL_HEADER.DllCharacteristics)                             # DllCharacteristics
    res.append(pe.OPTIONAL_HEADER.SizeOfStackReserve)                             # SizeOfStackReserve
    res.append(pe.OPTIONAL_HEADER.SizeOfStackCommit)                              # SizeOfStackCommit
    res.append(pe.OPTIONAL_HEADER.SizeOfHeapReserve)                              # SizeOfHeapReserve
    res.append(pe.OPTIONAL_HEADER.SizeOfHeapCommit)                               # SizeOfHeapCommit
    res.append(pe.OPTIONAL_HEADER.LoaderFlags)                                    # LoaderFlags
    res.append(pe.OPTIONAL_HEADER.NumberOfRvaAndSizes)                            # NumberOfRvaAndSizes
    res.append(len(pe.sections))                                                  
    entropy = list(map(lambda x:x.get_entropy(), pe.sections))                    # SectionsNb
    res.append(sum(entropy)/float(len(entropy)))                                  # SectionsMeanEntropy
    res.append(min(entropy))                                                      # SectionsMinEntropy
    res.append(max(entropy))                                                      # SectionsMaxEntropy
    raw_sizes = list(map(lambda x:x.SizeOfRawData, pe.sections))
    res.append(sum(raw_sizes)/float(len(raw_sizes)))                              # SectionsMeanRawsize
    res.append(min(raw_sizes))                                                    # SectionsMinRawsize
    res.append(max(raw_sizes))                                                    # SectionMaxRawsize
    virtual_sizes = list(map(lambda x:x.Misc_VirtualSize, pe.sections))
    res.append(sum(virtual_sizes)/float(len(virtual_sizes)))                      # SectionsMeanVirtualsize
    res.append(min(virtual_sizes))                                                # SectionsMinVirtualsize
    res.append(max(virtual_sizes))                                                # SectionMaxVirtualsize

    #Imports
    try:
        res.append(len(pe.DIRECTORY_ENTRY_IMPORT))                                # ImportsNbDLL
        imports = list(sum([x.imports for x in pe.DIRECTORY_ENTRY_IMPORT], []))   
        res.append(len(imports))                                                  # ImportsNb 
        res.append(len(list(map(lambda x:x.name is None, imports))))              # ImportsNbOrdinal
    except AttributeError:
        res.append(0)
        res.append(0)
        res.append(0)

    #Exports
    try:
        res.append(len(pe.DIRECTORY_ENTRY_EXPORT.symbols))                        # ExportNb
    except AttributeError:
        # No export
        res.append(0)
    
    resources= get_resources(pe)                                                  #Resources
    res.append(len(resources))                                                    # ResourcesNb
    if len(resources)> 0:
        entropy = list(map(lambda x:x[0], resources))
        res.append(sum(entropy)/float(len(entropy)))                              # ResourcesMeanSize
        res.append(min(entropy))                                                  # ResourcesMinEntropy
        res.append(max(entropy))                                                  # ResourcesMaxEntropy
        sizes = list(map(lambda x:x[1], resources))
        res.append(sum(sizes)/float(len(sizes)))                                  # ResourcesMeanSize
        res.append(min(sizes))                                                    # ResourcesMinSize
        res.append(max(sizes))                                                    # ResourcesMaxSize
    else:
        res.append(0)
        res.append(0)
        res.append(0)
        res.append(0)
        res.append(0)
        res.append(0)
                                                                                  # Load configuration size
    try:
        res.append(pe.DIRECTORY_ENTRY_LOAD_CONFIG.struct.Size)                    # LoadConfigurationSize
    except AttributeError:
        res.append(0)
                                                                                  # Version configuration size
    try:
        version_infos = get_version_info(pe)
        res.append(len(version_infos.keys()))                                     # VersionInformationSize
    except AttributeError:
        res.append(0)
    return res
fpath = "wildfire-test-pe-file.exe"                              # file path to predict if file is malware
file_extract = extract_infos(fpath)                              # takes in the file features and saves the list variable
arr = np.array(file_extract)                                     # converts the list to numpy array
arr = arr.reshape(1, 54)                                         # changes the 1D array to 2D array for prediction
print(arr)                                                       # prints the array
Result = clf.predict(arr)                                        # Predicts by entering the argument as a numpy array
Result
if Result==0:                                                    
    print("The file sample is malware")                          # prints if file is malware
else:
    print("The file sample is not malware")                      # prints if file is not malware